{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T14:05:03.816670Z",
     "start_time": "2020-05-08T14:05:03.810672Z"
    }
   },
   "source": [
    "# Phase I - Data Preparation and Modelling\n",
    "<hr>\n",
    "This is a <b>one time process</b> to generate vector embeddings for the document corpus using ensemble approach and store the model objects/configuration states for the searching purposes. There are 3 stages in this phase - <br>\n",
    "1. Data Preparation and Wrangling <br>\n",
    "2. Generation of embeddings for the document corpus for different techniques in ensemble approach<br>\n",
    "3. Saving the model objects and configuration for later use<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:41:07.004967Z",
     "start_time": "2020-05-08T08:41:06.999962Z"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T11:35:07.714919Z",
     "start_time": "2020-05-08T11:35:07.707882Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np \n",
    "import sys\n",
    "import os \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import scipy\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "from flask import Flask, render_template, jsonify, request\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import mixture\n",
    "\n",
    "from scipy import spatial\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import *\n",
    "from summarizer import Summarizer\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T11:35:09.079896Z",
     "start_time": "2020-05-08T11:35:08.964836Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.data.path.append('../bin/')\n",
    "nltk.download('stopwords',download_dir='../bin/', quiet=True)\n",
    "nltk.download('punkt',download_dir='../bin/', quiet=True)\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download('averaged_perceptron_tagger',download_dir='../bin/', quiet=True)\n",
    "nltk.download('wordnet',download_dir='../bin/', quiet=True)\n",
    "nltk.download('omw',download_dir='../bin/', quiet=True)\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Data loading and Cleaning\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:45:35.611010Z",
     "start_time": "2020-05-08T08:45:35.607005Z"
    }
   },
   "source": [
    "### Cleaning the abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T11:37:20.803647Z",
     "start_time": "2020-05-08T11:37:16.629731Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load the csv sources\n",
    "raw_md_data = pd.read_csv('../data/metadata.csv')\n",
    "raw_md_data.drop_duplicates(['abstract'], inplace=True) #drop duplicates by abstract\n",
    "raw_md_data.dropna(subset=['abstract'], inplace=True) #remove missing abstracts\n",
    "\n",
    "#Select required columns\n",
    "df = raw_md_data[['cord_uid', 'title', 'abstract']]\n",
    "clean_abstracts = df['abstract']\n",
    "\n",
    "#Storing only abstracts\n",
    "df.to_csv('../data/cleaned_abstracts.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data into JSON format for ingestion into Elasticsearch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T11:47:16.819810Z",
     "start_time": "2020-05-08T11:47:13.296783Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csvfile = open('../data/cleaned_abstracts.csv', 'r', encoding='utf-8')\n",
    "jsonfile = open('../data/cleaned_abstracts.json', 'w')\n",
    "\n",
    "fieldnames = (\"cord_uid\", \"title\", \"abstract\") #corresponding to the required columns\n",
    "reader = csv.DictReader(csvfile, fieldnames)\n",
    "for row in reader:\n",
    "    json.dump(row, jsonfile)\n",
    "    jsonfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T11:47:59.670010Z",
     "start_time": "2020-05-08T11:47:59.658964Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To clean the abstracts\n",
    "def clean_docs(doc_list):\n",
    "\n",
    "    doc_df = pd.DataFrame({'document':doc_list})\n",
    "\n",
    "    #Clean the data\n",
    "    # removing everything except alphabets`\n",
    "    doc_df['clean_doc'] = doc_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "    # removing short wordsstop_words = stopwords.words('english')\n",
    "    doc_df['clean_doc'] = doc_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "    # make all text lowercase\n",
    "    doc_df['clean_doc'] = doc_df['clean_doc'].apply(lambda x: x.lower())\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # tokenization\n",
    "    tokenized_doc = doc_df['clean_doc'].apply(lambda x: x.split())\n",
    "\n",
    "    # remove stop-words\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "    # de-tokenization\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(doc_df)):\n",
    "\n",
    "        try:\n",
    "            t = ' '.join(tokenized_doc[i])\n",
    "            detokenized_doc.append(t)\n",
    "        except:\n",
    "            print(f'Can not put {tokenized_doc[i]} back together')\n",
    "            detokenized_doc.append('')\n",
    "\n",
    "\n",
    "    detokenized_doc = np.array(detokenized_doc)\n",
    "\n",
    "    return detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:47:23.059599Z",
     "start_time": "2020-05-08T08:47:23.056595Z"
    }
   },
   "source": [
    "<a id=\"Modelling and embedding generation\"></a>\n",
    "## Modelling and embedding generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Topic-Modeling\"></a>\n",
    "### Topic Modeling with Cosine distance\n",
    "<hr>\n",
    "Topic modeling is a NLP unsupervised technique for assigning particular words to clusters. These clusters can be thought of as word clouds and contain similar terms. Latent semantic analysis (LSA) and latent Dirichlet allocation (LDA) are two of the most popular topic modeling methods. Here we use LSA for its computational speed, but LDA could also be considered here. Once again we use Cosine distance with the results of the topic modeling.\n",
    "<img src=\"https://i.ibb.co/23sp1Gb/tm-abstracts.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T11:48:04.530919Z",
     "start_time": "2020-05-08T11:48:04.521923Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS= 20\n",
    "\n",
    "def make_tm_output(doc_list,num_tf_idf_features=1000,num_compons=20):\n",
    "    \"\"\"\n",
    "    Make output for topic modeling\n",
    "    :param doc_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    detokenized_doc = clean_docs(doc_list)\n",
    "\n",
    "    # #Run the model\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',\n",
    "    max_features= num_tf_idf_features, # keep top 1000 terms\n",
    "    max_df = 0.25,\n",
    "    smooth_idf=True)\n",
    "\n",
    "\n",
    "    tfidf_output = vectorizer.fit_transform(detokenized_doc)\n",
    "\n",
    "    # SVD represent documents and terms in vectors\n",
    "    svd_model = TruncatedSVD(n_components=num_compons, algorithm='randomized', n_iter=100, random_state=42)\n",
    "\n",
    "    svd_model.fit(tfidf_output)\n",
    "\n",
    "    tm_output = svd_model.fit_transform(tfidf_output)\n",
    "    return tm_output, vectorizer,svd_model\n",
    "\n",
    "def tm_doc_embed(idx):\n",
    "    doc_vec = tm_output[idx,]\n",
    "    if(sum(doc_vec)==0): #to avoid all zeros (cosine similarity)\n",
    "        doc_vec = doc_vec + 1e-6\n",
    "    return doc_vec.tolist()\n",
    "\n",
    "def tm_query_embed(query):\n",
    "    clean_query = clean_docs([query])\n",
    "    tfidf_query_output = vectorizer.transform(clean_query)\n",
    "    target_vec = svd_model.transform(tfidf_query_output)\n",
    "    return target_vec.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T11:52:41.267852Z",
     "start_time": "2020-05-08T11:48:08.655883Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tm_output,vectorizer,svd_model = make_tm_output(list(clean_abstracts),num_compons=NUM_TOPICS)\n",
    "print('Time taken:', time.time()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models and vectors for the corpus for topic modeling\n",
    "with open('../models/tm_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(tm_output,f)\n",
    "    \n",
    "with open('../models/tm_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer,f)\n",
    "\n",
    "with open('../models/tm_svd_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svd_model,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"TF-IDF\"></a>\n",
    "### TF-IDF with Cosine distance\n",
    "<hr>\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a basic NLP method that determines the importance of an individual word relative to a document. That is, words are weighted based on how often then appear in a document and then inversely weighted based on how often they appear across a collection of documents.  Cosine distance (https://en.wikipedia.org/wiki/Cosine_similarity) is a common distance measure in the NLP literature and is used with many of the methods presented here. Cosine distance measures the difference in orientation. Thus, it is possible that two sentences or documents are far apart in Euclidean space but actually have similar orientations and are similar according to Cosine distance.\n",
    "<a name=\"some-id\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T11:57:28.917901Z",
     "start_time": "2020-05-08T11:57:28.893911Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tfidf_bow(doc_list):\n",
    "    #clean the docs\n",
    "    detokenized_doc = clean_docs(doc_list)\n",
    "    gen_docs = [[w.lower() for w in word_tokenize(text)] for text in detokenized_doc]\n",
    "\n",
    "    # create the dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "\n",
    "    # Create bag of words\n",
    "    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "    tf_idf = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tf_idf[corpus]\n",
    "    lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)\n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "    return dictionary, lsi, corpus_lsi\n",
    "\n",
    "def tfidf_doc_embed(idx):\n",
    "    doc_vec = [item[1] for item in corpus_lsi[idx]]\n",
    "    if not doc_vec: #to avoid empty vectors\n",
    "        doc_vec = 300*[1e-6]\n",
    "    return doc_vec\n",
    "\n",
    "def tfidf_query_embed(query):\n",
    "    detokenized_compare_doc = clean_docs([query])\n",
    "    gen_compare_docs = [[w.lower() for w in word_tokenize(text)] for text in detokenized_compare_doc]\n",
    "    query_doc_bow = dictionary.doc2bow(gen_compare_docs[0])\n",
    "    query_lsi = lsi[query_doc_bow]\n",
    "    query_vec_tfidf = [item[1] for item in query_lsi]\n",
    "    return query_vec_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T12:03:44.554251Z",
     "start_time": "2020-05-08T11:57:31.249891Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "dictionary, lsi, corpus_lsi = tfidf_bow(list(clean_abstracts))\n",
    "print('Time taken:', time.time()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save models and vectors for the corpus for TF-IDF\n",
    "with open('../models/tfidf_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(dictionary,f)\n",
    "    \n",
    "with open('../models/tfidf_lsi.pkl', 'wb') as f:\n",
    "    pickle.dump(lsi,f)\n",
    "\n",
    "with open('../models/tfidf_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_lsi,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"BERT-cos\"></a>\n",
    "### BERT with Cosine Distance \n",
    "<hr>\n",
    "Bidirectional Encoder Representations from Transformers (BERT) is a pre-trained model developed by Google. Unlike traditional RNNs or LSTMs, which only learn in one direction, BERT is trained in both directions and thus is better at understanding context. Once again we use Cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:49:48.075947Z",
     "start_time": "2020-05-08T08:49:48.046956Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One time task (already done and stored embeddings in 'models') \n",
    "#To generate BERT Embeddings for CORD dataset and store the embeddings\n",
    "def make_bert_embeddings(doc_list):\n",
    "    generic_bert_model = SentenceTransformer('../bin/models/')\n",
    "    return generic_bert_model.encode(doc_list,show_progress_bar=True)\n",
    "\n",
    "\n",
    "bert_corpus = list(clean_abstracts)\n",
    "\n",
    "start_time = time.time()\n",
    "# make the embeddings\n",
    "corpus_embed = make_bert_embeddings(list(bert_corpus))\n",
    "print(time.time()-start_time)\n",
    "\n",
    "with open('../models/bert_corpus_embed.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_embed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-08T12:04:26.467Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bert_doc_embed(idx):\n",
    "    return bert_embeddings[idx].tolist()\n",
    "\n",
    "def bert_query_embed(query):\n",
    "    generic_bert_model = SentenceTransformer('../bin/models/')\n",
    "    bert_vec = generic_bert_model.encode([query],show_progress_bar=True)\n",
    "    return bert_vec[0].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T14:15:25.910299Z",
     "start_time": "2020-05-08T14:15:25.906298Z"
    }
   },
   "source": [
    "# Phase II - Indexing, Searching and Summarization\n",
    "<hr> \n",
    "<b>Query based recurring tasks</b> <br>\n",
    "Once Phase I completes, we index the vectors and documents in Elasticsearch and perform our user searches on the given index. There are 5 stages in this phase - <br>\n",
    "1. Data/Embedding Ingestion into Elasticsearch as dense vectors <br>\n",
    "2. Generation of embeddings for user search input <br>\n",
    "3. Matching (user input embeddings and document vectors) using Dense Vector API and score using cosine similarity <br>\n",
    "4. Summarizing the relevant retrieved documents using BERT summarizer <br>\n",
    "5. Displaying search results in the Flask search UI <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Searching in Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Indexing data in ES\n",
    "def index_data():\n",
    "    print(\"Creating the index.\")\n",
    "    client.indices.delete(index=INDEX_NAME, ignore=[404])\n",
    "\n",
    "    with open(INDEX_FILE) as index_file:\n",
    "        source = index_file.read().strip()\n",
    "        client.indices.create(index=INDEX_NAME, body=source)\n",
    "\n",
    "    docs = []\n",
    "    count = 0\n",
    "\n",
    "    with open(DATA_FILE) as data_file:\n",
    "        for line in data_file:\n",
    "            line = line.strip()\n",
    "\n",
    "            doc = json.loads(line)\n",
    "        \n",
    "            docs.append(doc)\n",
    "            count += 1\n",
    "\n",
    "            if count % BATCH_SIZE == 0:\n",
    "                index_batch(docs,count)\n",
    "                docs = []\n",
    "                print(\"Indexed {} documents.\".format(count))\n",
    "\n",
    "        if docs:\n",
    "            index_batch(docs,count)\n",
    "            print(\"Indexed {} documents.\".format(count))\n",
    "\n",
    "    client.indices.refresh(index=INDEX_NAME)\n",
    "    print(\"Done indexing.\")\n",
    "\n",
    "def index_batch(docs,count):\n",
    "    requests = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        request = doc\n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        request[\"tm_doc_vec\"] = tm_doc_embed(i+count-BATCH_SIZE)\n",
    "        request[\"tfidf_doc_vec\"] = tfidf_doc_embed(i+count-BATCH_SIZE)\n",
    "        request[\"bert_doc_vec\"] = bert_doc_embed(i+count-BATCH_SIZE)\n",
    "        requests.append(request)\n",
    "    bulk(client, requests)\n",
    "\n",
    "\"\"\"Intermediate func (not used independently anymore)\n",
    "Created to test the code from within notebook and now, the same functionality is implemented in flask code\"\"\" \n",
    "\n",
    "def handle_query():\n",
    "    query = input(\"Enter query: \")\n",
    "\n",
    "    embedding_start = time.time()\n",
    "    tm_query_vec = tm_query_embed(query)\n",
    "    tfidf_query_vec = tfidf_query_embed(query)\n",
    "    bert_query_vec = bert_query_embed(query)\n",
    "    embedding_time = time.time() - embedding_start\n",
    "\n",
    "    script_query = {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\"match_all\": {}},\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.tm_qv, doc['tm_doc_vec']) + cosineSimilarity(params.tfidf_qv, doc['tfidf_doc_vec']) + cosineSimilarity(params.bert_qv, doc['bert_doc_vec']) + 3.0\",\n",
    "                \"params\": {\"tm_qv\": tm_query_vec, \"tfidf_qv\": tfidf_query_vec, \"bert_qv\": bert_query_vec}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_start = time.time()\n",
    "    response = client.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\": {\"includes\": [\"title\", \"abstract\", \"url\", \"authors\"]}\n",
    "        }\n",
    "    )\n",
    "    search_time = time.time() - search_start\n",
    "\n",
    "    print()\n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"]))\n",
    "    print(\"embedding time: {:.2f} ms\".format(embedding_time * 1000))\n",
    "    print(\"search time: {:.2f} ms\".format(search_time * 1000))\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"]))\n",
    "        print(hit[\"_source\"])\n",
    "        print()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Wrapper to run ingestion to ES\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T08:56:57.816717Z",
     "start_time": "2020-05-08T08:56:57.813712Z"
    }
   },
   "source": [
    "### Indexing and Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"docs\"\n",
    "INDEX_FILE = \"../resources/index.json\"\n",
    "\n",
    "DATA_FILE = \"../data/cleaned_abstracts.json\"\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "SEARCH_SIZE = 5  \n",
    "\n",
    "GPU_LIMIT = 0.5\n",
    "\n",
    "client = Elasticsearch()\n",
    "\n",
    "start_time = time.time()\n",
    "index_data()\n",
    "print('Time taken for indexing:', time.time()-start_time, 'seconds')\n",
    "\n",
    "#run_query_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T14:51:01.889306Z",
     "start_time": "2020-05-08T14:51:01.885312Z"
    }
   },
   "source": [
    "## Loading Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models and vectors for topic modeling\n",
    "with open('models/tm_vectors.pkl', 'rb') as f:\n",
    "    tm_output = pickle.load(f)\n",
    "    \n",
    "with open('models/tm_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "with open('models/tm_svd_model.pkl', 'rb') as f:\n",
    "    svd_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load models and vectors for TF-IDF\n",
    "with open('models/tfidf_dict.pkl', 'rb') as f:\n",
    "    dictionary = pickle.load(f)\n",
    "    \n",
    "with open('models/tfidf_lsi.pkl', 'rb') as f:\n",
    "    lsi = pickle.load(f)\n",
    "\n",
    "with open('models/tfidf_vectors.pkl', 'rb') as f:\n",
    "    corpus_lsi = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T12:04:22.702086Z",
     "start_time": "2020-05-08T12:04:21.948751Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load already saved BERT embeddings from models folder\n",
    "with open('../models/bert_corpus_embed.pkl', 'rb') as f:\n",
    "    bert_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "custom_config = AutoConfig.from_pretrained('../bin/bert/')\n",
    "custom_config.output_hidden_states = True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained('../bin/bert')\n",
    "custom_model = AutoModel.from_pretrained('../bin/bert', config=custom_config)\n",
    "\n",
    "#Pass the concatenated text string from top docs for summarisation\n",
    "def get_summary(text):\n",
    "    body = str(text)\n",
    "    model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "    summary = model(body, min_length=60)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Call to summarizer\n",
    "output_text = \"\"\n",
    "for hit in response['hits']['hits']:\n",
    "    selected_text = hit['_source']['abstract']\n",
    "    output_text = output_text + \" \" + selected_text\n",
    "\n",
    "print(\"Summary derived from the top 5 abstracts - \\n {}\".format(get_summary(output_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching via Flask Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "#     return render_template('testpage.html')\n",
    "\n",
    "\n",
    "@app.route('/search')\n",
    "def analyser():\n",
    "    query = request.args.get('q')\n",
    "    \n",
    "    if query:\n",
    "        embedding_start = time.time()\n",
    "        tm_query_vec = tm_query_embed(query)\n",
    "        tfidf_query_vec = tfidf_query_embed(query)\n",
    "        bert_query_vec = bert_query_embed(query)\n",
    "        embedding_time = time.time() - embedding_start\n",
    "\n",
    "        script_query = {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.tm_qv, doc['tm_doc_vec']) + cosineSimilarity(params.tfidf_qv, doc['tfidf_doc_vec']) + cosineSimilarity(params.bert_qv, doc['bert_doc_vec']) + 3.0\",\n",
    "                    \"params\": {\"tm_qv\": tm_query_vec, \"tfidf_qv\": tfidf_query_vec, \"bert_qv\": bert_query_vec}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        search_start = time.time()\n",
    "        response = client.search(\n",
    "            index=INDEX_NAME,\n",
    "            body={\n",
    "                \"size\": SEARCH_SIZE,\n",
    "                \"query\": script_query,\n",
    "                \"_source\": {\"includes\": [\"title\", \"abstract\"]}\n",
    "            }\n",
    "        )\n",
    "        search_time = time.time() - search_start\n",
    "\n",
    "        #Call to summarizer\n",
    "        output_text = \"\"\n",
    "        for hit in response['hits']['hits']:\n",
    "            selected_text = hit['_source']['abstract']\n",
    "            output_text = output_text + \" \" + selected_text\n",
    "\n",
    "        summary = get_summary(output_text)\n",
    "\n",
    "        result_disp = {\"top_docs\" : response, \"summary\" : summary}\n",
    "        return jsonify(result_disp)\n",
    "    \n",
    "    else:\n",
    "        return \n",
    "app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
